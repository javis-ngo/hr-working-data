{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb00cf0aee039fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T10:29:46.496508Z",
     "start_time": "2025-08-22T10:29:41.552781Z"
    },
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate key detected in hiring on 'Emp id': 2 rows across 1 keys\n",
      "Duplicate key detected in lineup on 'Số CMND/CCCD:': 587 rows across 275 keys\n",
      "Đã tạo file: hr_files\\HR_buiminhph.xlsx với 6 hàng\n",
      "Đã tạo file: hr_files\\HR_ho_thanh.xlsx với 1 hàng\n",
      "Đã tạo file: hr_files\\HR_myduyen_ly.xlsx với 6 hàng\n",
      "{'read_s': 5.149, 'merge_s': 0.008, 'map_s': 0.008, 'enrich_s': 0.001, 'write_master_s': 0.065, 'write_hr_s': 0.152, 'total_s': 5.422, 'rows_hiring': 22, 'rows_final': 22}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# =====================\n",
    "# Configurations\n",
    "# =====================\n",
    "TEMPLATE_FILE_PATH = os.path.abspath(\"workday_data.xlsx\")\n",
    "OUTPUT_MASTER_FILE = \"master_data.xlsx\"\n",
    "HR_OUTPUT_DIR = \"hr_files\"\n",
    "LOGS_DIR = \"logs\"\n",
    "\n",
    "MASTER_SHEET = \"Masterdata_PSteam\"\n",
    "EMPLOYEE_MASTER_SHEET = \"Employee Master\"\n",
    "HIRING_SHEET = \"Hiring_data\"\n",
    "LINEUP_SHEET = \"LINEUP_PASS_IMPORT\"\n",
    "# TERMINATION_SHEET = \"Termination\"  # Currently unused\n",
    "PERMISSION_SHEET = \"Permission\"\n",
    "\n",
    "SKIPROWS = {\n",
    "    MASTER_SHEET: 11,\n",
    "    EMPLOYEE_MASTER_SHEET: 11,\n",
    "    HIRING_SHEET: 1,\n",
    "    LINEUP_SHEET: 0,\n",
    "    PERMISSION_SHEET: 0,\n",
    "}\n",
    "\n",
    "# Only append existing Masterdata_PSteam content if required\n",
    "APPEND_EXISTING_MASTER = False\n",
    "\n",
    "# Column mapping from source columns -> target schema columns\n",
    "COLUMN_MAPPING = {\n",
    "    'Emp id': 'EID',\n",
    "    'Hire Date (DD-MMM-YYYY)': 'DOJ',\n",
    "    'ORIGINAL_DATE_OF_HIRE': 'DOJ',\n",
    "    'FULL_NAME': 'EMPLOYEE FULL NAME_EN',\n",
    "    'Họ tên': 'EMPLOYEE FULL NAME_VN',\n",
    "    'Gender': 'GENDER',\n",
    "    'Giới tính': 'GENDER',\n",
    "    'DOB (DD-MMM-YYYY)': 'DOB',\n",
    "    'Dân tộc': 'ETHNIC',\n",
    "    'Nơi sinh': 'BIRTH PLACE',\n",
    "    'Số CMND/CCCD:': 'ID CARD',\n",
    "    'Ngày cấp CMND/CCCD': 'ISSUED DATE',\n",
    "    'Nơi cấp CCCD/CMND': 'ISSUED PLACE',\n",
    "    'Số CMND Cũ (Nếu Có)': 'OLD ID CARD NO.',\n",
    "    'Địa chỉ thường trú:': 'PERMANENT ADDRESS',\n",
    "    'Địa chỉ tạm trú': 'TEMPORARY ADDRESS',\n",
    "    'Highest Education': 'EDUCATION',\n",
    "    'Tình trạng hôn nhân': 'MARITAL STATUS',\n",
    "    'Phone Number': 'MOBILE PHONE NO',\n",
    "    'Email - Work': 'CNX EMAIL',\n",
    "    'Home E-mail address': 'PERSONAL EMAIL',\n",
    "    'Số điện thoại người liên hệ trong trường hợp khẩn cấp': 'EMERGENCY CONTACT',\n",
    "    'Số Bảo Hiểm Xã Hội': 'SOCIAL INSURANCE NUMBER',\n",
    "    'Mã số thuế': 'PERSONAL TAX CODE',\n",
    "    'Số tài khoản ngân hàng nhận lương': 'BANK ACCOUNT',\n",
    "    'Tên ngân hàng': 'BANK NAME',\n",
    "    'Base Pay': 'BASE SALARY',\n",
    "    'Complexity Allowance': 'COMPLEXITY ALLOWANCE',  # fixed typo\n",
    "    'Position Allowance': 'POSITION ALLOWANCE',\n",
    "    'Meal Allowance - Monthly': 'MEAL ALLOWANCE',\n",
    "    'Business Title': 'BUSINESS_TITLE_EN',\n",
    "    'Vị trí': 'BUSINESS_TITLE_VN',\n",
    "    'SUPERVISOR_FULL_NAME': 'SUPERVISOR',\n",
    "    'Career Level': 'CAREER LEVEL',\n",
    "    'Họ và tên chủ hộ': 'Họ tên chủ hộ',\n",
    "    'Giới tính chủ hộ': 'Giới tính chủ hộ',\n",
    "    'Ngày sinh của chủ hộ': 'DOB chủ hộ',\n",
    "    'Mối quan hệ giữa chủ hộ và bạn': 'Mối quan hệ giữa NLĐ với chủ hộ',\n",
    "    'MSA Client': 'MSA Client',\n",
    "    'Contract ID': 'PROBATION CONTRACT NO',\n",
    "    'Process': 'PROJECT',\n",
    "    'Start Date (DD-MMM-YYYY)': 'FROM',\n",
    "    'End Date (DD-MMM-YYYY)': 'TO',\n",
    "    'Bệnh viện muốn đăng ký BHYT': 'HOSPITAL NAME',  # trim trailing space\n",
    "    'Legislation Code / Country': 'NATIONALITY',\n",
    "}\n",
    "\n",
    "# =====================\n",
    "# Logging\n",
    "# =====================\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(LOGS_DIR, 'merge_log.txt'),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "def log_and_print(message):\n",
    "    logging.info(message)\n",
    "    print(message)\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Utility functions\n",
    "# =====================\n",
    "\n",
    "def read_sheet(path, sheet_name, skiprows=0, usecols=None):\n",
    "    return pd.read_excel(\n",
    "        path,\n",
    "        engine='openpyxl',\n",
    "        dtype=object,\n",
    "        sheet_name=sheet_name,\n",
    "        skiprows=skiprows,\n",
    "        usecols=usecols\n",
    "    )\n",
    "\n",
    "\n",
    "def get_master_schema_columns(path):\n",
    "    # Read header only to get schema columns, avoid loading entire sheet\n",
    "    df_header = pd.read_excel(\n",
    "        path,\n",
    "        engine='openpyxl',\n",
    "        sheet_name=MASTER_SHEET,\n",
    "        skiprows=SKIPROWS[MASTER_SHEET],\n",
    "        nrows=0\n",
    "    )\n",
    "    schema_cols = [col for col in df_header.columns if not str(col).startswith('Unnamed')]\n",
    "    return schema_cols\n",
    "\n",
    "\n",
    "def normalize_emp_id(series):\n",
    "    if series is None:\n",
    "        return series\n",
    "    return series.astype(str).str.strip()\n",
    "\n",
    "\n",
    "def normalize_national_id(series):\n",
    "    if series is None:\n",
    "        return series\n",
    "    # keep digits only for national id, remove spaces and punctuation\n",
    "    return series.astype(str).str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "\n",
    "def safe_merge(left, right, left_on, right_on, how='left', validate=None, label=\"\"):\n",
    "    # Optional validation of key uniqueness\n",
    "    if validate is not None:\n",
    "        try:\n",
    "            return left.merge(right, left_on=left_on, right_on=right_on, how=how, validate=validate)\n",
    "        except Exception as exc:\n",
    "            log_and_print(f\"Merge validation failed for {label}: {exc}\")\n",
    "            return left.merge(right, left_on=left_on, right_on=right_on, how=how)\n",
    "    return left.merge(right, left_on=left_on, right_on=right_on, how=how)\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Main pipeline\n",
    "# =====================\n",
    "start_ts = time.time()\n",
    "try:\n",
    "    # 1) Read minimal inputs\n",
    "    t0 = time.time()\n",
    "    master_schema_cols = get_master_schema_columns(TEMPLATE_FILE_PATH)\n",
    "    hiring_df = read_sheet(TEMPLATE_FILE_PATH, HIRING_SHEET, SKIPROWS[HIRING_SHEET])\n",
    "    emp_master_df = read_sheet(TEMPLATE_FILE_PATH, EMPLOYEE_MASTER_SHEET, SKIPROWS[EMPLOYEE_MASTER_SHEET])\n",
    "    lineup_df = read_sheet(TEMPLATE_FILE_PATH, LINEUP_SHEET, SKIPROWS[LINEUP_SHEET])\n",
    "    permission_df = read_sheet(TEMPLATE_FILE_PATH, PERMISSION_SHEET, SKIPROWS[PERMISSION_SHEET])\n",
    "\n",
    "    read_time = time.time() - t0\n",
    "\n",
    "    # 2) Normalize join keys\n",
    "    if 'Emp id' in hiring_df.columns:\n",
    "        hiring_df['Emp id'] = normalize_emp_id(hiring_df['Emp id'])\n",
    "    if 'EMPLOYEE_NUMBER' in emp_master_df.columns:\n",
    "        emp_master_df['EMPLOYEE_NUMBER'] = normalize_emp_id(emp_master_df['EMPLOYEE_NUMBER'])\n",
    "\n",
    "    if 'National ID (SSN/SIN) (National Identifiers)' in hiring_df.columns:\n",
    "        hiring_df['National ID (SSN/SIN) (National Identifiers)'] = normalize_national_id(\n",
    "            hiring_df['National ID (SSN/SIN) (National Identifiers)']\n",
    "        )\n",
    "    if 'Số CMND/CCCD:' in lineup_df.columns:\n",
    "        lineup_df['Số CMND/CCCD:'] = normalize_national_id(lineup_df['Số CMND/CCCD:'])\n",
    "\n",
    "    # 2.1) Deduplicate keys prior to merge\n",
    "    def safe_filename(text):\n",
    "        return re.sub(r'[<>:\"/\\\\|?*]+', '_', str(text))\n",
    "\n",
    "    def report_dups(df, key, name):\n",
    "        if key in df.columns:\n",
    "            dup = df[df.duplicated(subset=[key], keep=False) & df[key].notna()]\n",
    "            if not dup.empty:\n",
    "                log_and_print(f\"Duplicate key detected in {name} on '{key}': {dup.shape[0]} rows across {dup[key].nunique()} keys\")\n",
    "                dups_dir = os.path.join(LOGS_DIR, 'dups')\n",
    "                os.makedirs(dups_dir, exist_ok=True)\n",
    "                out = os.path.join(dups_dir, f\"dups_{safe_filename(name)}_{safe_filename(key)}.csv\")\n",
    "                dup.to_csv(out, index=False)\n",
    "                # Keep the first occurrence by default\n",
    "                df.drop_duplicates(subset=[key], keep='first', inplace=True)\n",
    "        return df\n",
    "\n",
    "    hiring_df = report_dups(hiring_df, 'Emp id', 'hiring')\n",
    "    emp_master_df = report_dups(emp_master_df, 'EMPLOYEE_NUMBER', 'employee_master')\n",
    "    lineup_df = report_dups(lineup_df, 'Số CMND/CCCD:', 'lineup')\n",
    "\n",
    "    # 3) Merge hiring with employee master\n",
    "    t1 = time.time()\n",
    "    merged_first = safe_merge(\n",
    "        hiring_df,\n",
    "        emp_master_df,\n",
    "        left_on='Emp id',\n",
    "        right_on='EMPLOYEE_NUMBER',\n",
    "        how='left',\n",
    "        validate='many_to_one',  # many hires to one employee master row\n",
    "        label='hiring_emp_master'\n",
    "    )\n",
    "    # remove suffixes\n",
    "    merged_first = merged_first.loc[:, ~merged_first.columns.str.endswith('_y')]\n",
    "    merged_first.columns = [c.replace('_x', '') for c in merged_first.columns]\n",
    "\n",
    "    # 4) Merge with lineup by national id\n",
    "    merged_second = safe_merge(\n",
    "        merged_first,\n",
    "        lineup_df,\n",
    "        left_on='National ID (SSN/SIN) (National Identifiers)',\n",
    "        right_on='Số CMND/CCCD:',\n",
    "        how='left',\n",
    "        validate='many_to_one',  # many hires to one lineup record\n",
    "        label='with_lineup'\n",
    "    )\n",
    "    merged_second = merged_second.loc[:, ~merged_second.columns.str.endswith('_y')]\n",
    "    merged_second.columns = [c.replace('_x', '') for c in merged_second.columns]\n",
    "\n",
    "    merge_time = time.time() - t1\n",
    "\n",
    "    # 5) Map columns to target schema\n",
    "    t2 = time.time()\n",
    "    merged_second = merged_second.dropna(how='all')\n",
    "\n",
    "    # Build mapped dataframe according to schema\n",
    "    mapped = {}\n",
    "    for source_col, target_col in COLUMN_MAPPING.items():\n",
    "        if source_col in merged_second.columns:\n",
    "            mapped[target_col] = merged_second[source_col]\n",
    "    mapped_df = pd.concat(mapped, axis=1) if mapped else pd.DataFrame(columns=master_schema_cols)\n",
    "\n",
    "    # Ensure all schema columns exist and are ordered\n",
    "    for col in master_schema_cols:\n",
    "        if col not in mapped_df.columns:\n",
    "            mapped_df[col] = pd.NA\n",
    "\n",
    "    # Convert DOJ if it looks like an Excel serial number\n",
    "    if 'DOJ' in mapped_df.columns:\n",
    "        ser = mapped_df['DOJ']\n",
    "        try:\n",
    "            # try numeric conversion; values that fail remain NaN\n",
    "            ser_num = pd.to_numeric(ser, errors='coerce')\n",
    "            # Heuristic: Excel serials are typically > 20000\n",
    "            mask_serial = ser_num.notna() & (ser_num > 20000)\n",
    "            mapped_df.loc[mask_serial, 'DOJ'] = pd.to_datetime(ser_num[mask_serial], unit='D', origin='1899-12-30')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    mapped_df = mapped_df[master_schema_cols]\n",
    "\n",
    "    # Optionally append existing master rows (expensive)\n",
    "    if APPEND_EXISTING_MASTER:\n",
    "        master_existing = read_sheet(TEMPLATE_FILE_PATH, MASTER_SHEET, SKIPROWS[MASTER_SHEET])\n",
    "        master_existing = master_existing.loc[:, ~master_existing.columns.str.startswith('Unnamed')]\n",
    "        # Do not drop all NA rows globally to avoid losing partial records\n",
    "        master_df = pd.concat([master_existing, mapped_df], ignore_index=True)\n",
    "    else:\n",
    "        master_df = mapped_df.copy()\n",
    "\n",
    "    map_time = time.time() - t2\n",
    "\n",
    "    # 6) Enrich with permission on \"MSA Client\"\n",
    "    t3 = time.time()\n",
    "    if 'MSA Client' in master_df.columns and 'MSA Client' in permission_df.columns:\n",
    "        final_df = master_df.merge(permission_df, on='MSA Client', how='left')\n",
    "    else:\n",
    "        final_df = master_df\n",
    "        log_and_print(\"Warning: 'MSA Client' not found in one of the datasets; skipping permission merge.\")\n",
    "\n",
    "    enrich_time = time.time() - t3\n",
    "\n",
    "    # 7) Write master output using fast writer\n",
    "    t4 = time.time()\n",
    "    with pd.ExcelWriter(OUTPUT_MASTER_FILE, engine='xlsxwriter') as writer:\n",
    "        final_df.to_excel(writer, sheet_name='master_data', index=False)\n",
    "    write_master_time = time.time() - t4\n",
    "\n",
    "    # 8) Export HR files by SSO (skip NaN SSO)\n",
    "    t5 = time.time()\n",
    "    if 'SSO' not in final_df.columns:\n",
    "        raise ValueError(\"Cột 'SSO' không tồn tại trong master_data\")\n",
    "\n",
    "    os.makedirs(HR_OUTPUT_DIR, exist_ok=True)\n",
    "    groups = final_df.dropna(subset=['SSO']).groupby('SSO', dropna=True)\n",
    "    for sso, hr_data in groups:\n",
    "        safe_sso = re.sub(r'[<>:\"/\\\\|?*]', '_', str(sso))\n",
    "        out_path = os.path.join(HR_OUTPUT_DIR, f\"HR_{safe_sso}.xlsx\")\n",
    "        with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:\n",
    "            hr_data.to_excel(writer, index=False)\n",
    "        print(f\"Đã tạo file: {out_path} với {len(hr_data)} hàng\")\n",
    "    write_hr_time = time.time() - t5\n",
    "\n",
    "    total_time = time.time() - start_ts\n",
    "\n",
    "    # 9) Report timings\n",
    "    timings = {\n",
    "        'read_s': round(read_time, 3),\n",
    "        'merge_s': round(merge_time, 3),\n",
    "        'map_s': round(map_time, 3),\n",
    "        'enrich_s': round(enrich_time, 3),\n",
    "        'write_master_s': round(write_master_time, 3),\n",
    "        'write_hr_s': round(write_hr_time, 3),\n",
    "        'total_s': round(total_time, 3),\n",
    "        'rows_hiring': int(len(hiring_df)),\n",
    "        'rows_final': int(len(final_df)),\n",
    "    }\n",
    "    log_and_print(str(timings))\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(\"Pipeline failed\")\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
